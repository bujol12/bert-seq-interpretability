[config]
experiment_name = final_soft_attention
dataset = conll10
model_name = roberta-base
max_seq_length = 128
per_device_train_batch_size = 16
per_device_eval_batch_size = 64
num_train_epochs = 20
warmup_ratio = 0.1
learning_rate = 2e-5
weight_decay = 0.1
seed = 15
adam_epsilon = 1e-7
test_label_dummy = test
make_all_labels_equal_max = True
is_seq_class = True
lowercase = True
gradient_accumulation_steps = 1
save_steps = 500
logging_steps = 500
output_dir = models/{experiment_name}/{model_name}/{dataset_name}/{datetime}/
do_mask_words = False
mask_prob = 0.0
hid_to_attn_dropout = 0.10
attention_evidence_size = 100
final_hidden_layer_size = 300
initializer_name = glorot
attention_activation = soft
soft_attention = True
soft_attention_gamma = 0.1
soft_attention_alpha = 0.1
square_attention = True
freeze_bert_layers_up_to = 0
zero_n = 0
zero_delta = 0.0